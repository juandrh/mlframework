{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\nfrom xgboost import XGBRegressor, DMatrix\n\nimport matplotlib.pyplot as plt\n%matplotlib inline            \nimport seaborn as sns\n\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, DeltaYStopper\nfrom skopt.space import Real, Categorical, Integer\n\n# Model selection\nfrom sklearn.model_selection import KFold\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T09:25:56.176145Z","iopub.execute_input":"2021-08-19T09:25:56.176442Z","iopub.status.idle":"2021-08-19T09:25:57.290912Z","shell.execute_reply.started":"2021-08-19T09:25:56.176373Z","shell.execute_reply":"2021-08-19T09:25:57.290111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/30days-kfolds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndf_test = df_test[useful_features]\n\n\n\n\nfinal_predictions = []\nscores=[]\nfor fold in range(6):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    # standarization\n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0,predictor='gpu_predictor',objective='reg:squarederror',)\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold,rmse)\n    scores.append(rmse)\n    \nprint (np.mean(scores),np.std(scores))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T01:28:27.219666Z","iopub.execute_input":"2021-08-19T01:28:27.219985Z","iopub.status.idle":"2021-08-19T01:28:53.331784Z","shell.execute_reply.started":"2021-08-19T01:28:27.219957Z","shell.execute_reply":"2021-08-19T01:28:53.330854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0.7251556496577392 0.0019415578945857727<br>\n0.7250257964049016 0.001937036680455837 <br>\n0.725143199009303 0.00192218924080936<br>\n","metadata":{}},{"cell_type":"code","source":"# polinomial features\n \ndf = pd.read_csv(\"../input/30days-kfolds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndf_test = df_test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\ntrain_poly = poly.fit_transform(df[numerical_cols])\ntest_poly = poly.fit_transform(df_test[numerical_cols])\n\ndf_poly = pd.DataFrame(train_poly, columns= [f\"poly_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns= [f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\ndf = pd.concat([df, df_poly], axis = 1)\ndf_test = pd.concat([df_test, df_test_poly], axis = 1)\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_predictions = []\nscores=[]\nfor fold in range(6):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n \n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0,predictor='gpu_predictor')\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold,rmse)\n    scores.append(rmse)\n    \nprint (np.mean(scores),np.std(scores))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T02:50:11.00931Z","iopub.execute_input":"2021-08-19T02:50:11.009654Z","iopub.status.idle":"2021-08-19T02:50:52.521563Z","shell.execute_reply.started":"2021-08-19T02:50:11.009627Z","shell.execute_reply":"2021-08-19T02:50:52.520643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# polinomial features\n \ndf = pd.read_csv(\"../input/30days-kfolds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndf_test = df_test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\ntrain_poly = poly.fit_transform(df[numerical_cols])\ntest_poly = poly.fit_transform(df_test[numerical_cols])\n\ndf_poly = pd.DataFrame(train_poly, columns= [f\"poly_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns= [f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\ndf = pd.concat([df, df_poly], axis = 1)\ndf_test = pd.concat([df_test, df_test_poly], axis = 1)\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_predictions = []\nscores=[]\nfor fold in range(6):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n \n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0,predictor='gpu_predictor')\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold,rmse)\n    scores.append(rmse)\n    \nprint (np.mean(scores),np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T02:50:52.523007Z","iopub.execute_input":"2021-08-19T02:50:52.523515Z","iopub.status.idle":"2021-08-19T02:52:23.022095Z","shell.execute_reply.started":"2021-08-19T02:50:52.523478Z","shell.execute_reply":"2021-08-19T02:52:23.02122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"degree 2= 0.7276382385675535 0.001629719053959<br>\ndegree 3= 0.7297184719788427 0.002042522648024433","metadata":{}},{"cell_type":"markdown","source":"## Binning the numerical feautures \npd.cut","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-19T02:13:49.287194Z","iopub.execute_input":"2021-08-19T02:13:49.287522Z","iopub.status.idle":"2021-08-19T02:13:49.298163Z","shell.execute_reply.started":"2021-08-19T02:13:49.287493Z","shell.execute_reply":"2021-08-19T02:13:49.296932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(7, 2,figsize=(28, 40))\nsns.distplot(ax=axes[0, 0],a=df['cont0'], kde=False)\nsns.distplot(ax=axes[0, 1],a=df['cont1'], kde=False)\nsns.distplot(ax=axes[1, 0],a=df['cont2'], kde=False)\nsns.distplot(ax=axes[1, 1],a=df['cont3'], kde=False)\nsns.distplot(ax=axes[2, 0],a=df['cont4'], kde=False)\nsns.distplot(ax=axes[2, 1],a=df['cont5'], kde=False)\nsns.distplot(ax=axes[3, 0],a=df['cont6'], kde=False)\nsns.distplot(ax=axes[3, 1],a=df['cont7'], kde=False)\nsns.distplot(ax=axes[4 ,0],a=df['cont8'], kde=False)\nsns.distplot(ax=axes[4, 1],a=df['cont9'], kde=False)\nsns.distplot(ax=axes[5, 0],a=df['cont10'], kde=False)\nsns.distplot(ax=axes[5, 1],a=df['cont11'], kde=False)\nsns.distplot(ax=axes[6, 0],a=df['cont12'], kde=False)\nsns.distplot(ax=axes[6, 1],a=df['cont13'], kde=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T02:28:14.027497Z","iopub.execute_input":"2021-08-19T02:28:14.027829Z","iopub.status.idle":"2021-08-19T02:28:17.609259Z","shell.execute_reply.started":"2021-08-19T02:28:14.027785Z","shell.execute_reply":"2021-08-19T02:28:17.608218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quantile_list=[0,0.25,0.5,0.75,1.0]\nquantile=df['cont0'].quantile(quantile_list)\nquantile","metadata":{"execution":{"iopub.status.busy":"2021-08-19T02:31:49.317295Z","iopub.execute_input":"2021-08-19T02:31:49.317629Z","iopub.status.idle":"2021-08-19T02:31:49.337053Z","shell.execute_reply.started":"2021-08-19T02:31:49.3176Z","shell.execute_reply":"2021-08-19T02:31:49.335872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']\ndf['cont0_quantile_range'] = pd.qcut(\n                                            df['cont0'], \n                                            q=quantile_list)\ndf['cont0_quantile_label'] = pd.qcut(\n                                            df['cont0'], \n                                            q=quantile_list,       \n                                            labels=quantile_labels)\ndf\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T02:40:34.851081Z","iopub.execute_input":"2021-08-19T02:40:34.851411Z","iopub.status.idle":"2021-08-19T02:40:35.099042Z","shell.execute_reply.started":"2021-08-19T02:40:34.851382Z","shell.execute_reply":"2021-08-19T02:40:35.098064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one hot encoding\n\ndf = pd.read_csv(\"../input/30days-kfolds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndf_test = df_test[useful_features]\n\n\n\n\nfinal_predictions = []\nscores=[]\nfor fold in range(6):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ohe = preprocessing.OneHotEncoder(sparse=False,handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[object_cols])\n    xvalid_ohe = ohe.transform(xvalid[object_cols])\n    xtest_ohe= ohe.transform(xtest[object_cols])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns= [f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns= [f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n    xtest_ohe = pd.DataFrame(xtest_ohe, columns= [f\"ohe_{i}\" for i in range(xtest_ohe.shape[1])])\n\n    xtrain = pd.concat([xtrain, xtrain_ohe], axis = 1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis = 1)\n    xtest= pd.concat([xtest, xtest_ohe], axis = 1)\n    \n    xtrain = xtrain.drop(object_cols, axis = 1)\n    xvalid = xvalid.drop(object_cols, axis = 1)\n    xtest = xtest.drop(object_cols, axis = 1)    \n\n    # standarization\n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0,predictor='gpu_predictor')\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold,rmse)\n    scores.append(rmse)\n    \nprint (np.mean(scores),np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:22:01.615597Z","iopub.execute_input":"2021-08-19T03:22:01.615947Z","iopub.status.idle":"2021-08-19T03:22:33.704527Z","shell.execute_reply.started":"2021-08-19T03:22:01.615915Z","shell.execute_reply":"2021-08-19T03:22:33.703664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0.725143199009303 0.00192218924080936<br>\n0.7253373267677583 0.001674973296325871","metadata":{}},{"cell_type":"code","source":"# one hot encoding  + sttandarization of ohe Â¬ numerical\n\n\ndf = pd.read_csv(\"../input/30days-kfolds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndf_test = df_test[useful_features]\n\n\n\n\nfinal_predictions = []\nscores=[]\nfor fold in range(6):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ohe = preprocessing.OneHotEncoder(sparse=False,handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[object_cols])\n    xvalid_ohe = ohe.transform(xvalid[object_cols])\n    xtest_ohe= ohe.transform(xtest[object_cols])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns= [f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns= [f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n    xtest_ohe = pd.DataFrame(xtest_ohe, columns= [f\"ohe_{i}\" for i in range(xtest_ohe.shape[1])])\n\n    xtrain = pd.concat([xtrain, xtrain_ohe], axis = 1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis = 1)\n    xtest= pd.concat([xtest, xtest_ohe], axis = 1)\n    \n    xtrain = xtrain.drop(object_cols, axis = 1)\n    xvalid = xvalid.drop(object_cols, axis = 1)\n    xtest = xtest.drop(object_cols, axis = 1)    \n\n    # standarization\n    scaler = preprocessing.StandardScaler()\n    xtrain[:] = scaler.fit_transform(xtrain[:])\n    xvalid[:] = scaler.transform(xvalid[:])\n    xtest[:] = scaler.transform(xtest[:])\n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0,predictor='gpu_predictor')\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold,rmse)\n    scores.append(rmse)\n    \nprint (np.mean(scores),np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T09:26:23.57388Z","iopub.execute_input":"2021-08-19T09:26:23.574219Z","iopub.status.idle":"2021-08-19T09:26:56.511276Z","shell.execute_reply.started":"2021-08-19T09:26:23.574187Z","shell.execute_reply":"2021-08-19T09:26:56.509771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform cat to numerical using counts\n\ndf = pd.read_csv(\"../input/30days-kfolds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndf_test = df_test[useful_features]\n\nfor col in object_cols:\n    df[f\"cont_{col}\"] = df.groupby(col)[col].transform(\"count\")\n    df_test[f\"cont_{col}\"] = df_test.groupby(col)[col].transform(\"count\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith('cat')]\ndf_test = df_test[useful_features]    \n\nfinal_predictions = []\nscores=[]\nfor fold in range(6):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    \n    # standarization\n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0,predictor='gpu_predictor')\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold,rmse)\n    scores.append(rmse)\n    \nprint (np.mean(scores),np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:48:05.896613Z","iopub.execute_input":"2021-08-19T03:48:05.896995Z","iopub.status.idle":"2021-08-19T03:48:45.332056Z","shell.execute_reply.started":"2021-08-19T03:48:05.896965Z","shell.execute_reply":"2021-08-19T03:48:45.331173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0.7252247379444503 0.0018250341871516866 <br>\n\n0.7252235823355567 0.0017443383032877378","metadata":{}},{"cell_type":"markdown","source":"# Combine cat variables\ncat1_cat2\ndf[cat1] + \"-\" + df[cat2]\n\n# Combine cat variables + numerical using groupby (mean, max..) \ncat1_cat2\ndf[cat1] + \"-\" + df[cat2]","metadata":{}},{"cell_type":"code","source":"preds = np.mean(np.column_stack(final_predictions), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T09:27:10.271913Z","iopub.execute_input":"2021-08-19T09:27:10.272259Z","iopub.status.idle":"2021-08-19T09:27:10.285584Z","shell.execute_reply.started":"2021-08-19T09:27:10.27223Z","shell.execute_reply":"2021-08-19T09:27:10.28475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.target = preds\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T09:27:13.705578Z","iopub.execute_input":"2021-08-19T09:27:13.705898Z","iopub.status.idle":"2021-08-19T09:27:14.211272Z","shell.execute_reply.started":"2021-08-19T09:27:13.705869Z","shell.execute_reply":"2021-08-19T09:27:14.210406Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
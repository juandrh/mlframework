{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.8.10 64-bit"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.8.10","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import make_scorer\n","from xgboost import XGBRegressor, DMatrix\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline            \n","import seaborn as sns\n","\n","from time import time\n","import pprint\n","import joblib\n","from functools import partial\n","\n","# Suppressing warnings because of skopt verbosity\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Skopt functions\n","from skopt import BayesSearchCV\n","from skopt.callbacks import DeadlineStopper, DeltaYStopper\n","from skopt.space import Real, Categorical, Integer\n","\n","# Model selection\n","from sklearn.model_selection import KFold\n","\n","import optuna"],"metadata":{"execution":{"iopub.status.busy":"2021-08-19T09:25:56.176145Z","iopub.execute_input":"2021-08-19T09:25:56.176442Z","iopub.status.idle":"2021-08-19T09:25:57.290912Z","shell.execute_reply.started":"2021-08-19T09:25:56.176373Z","shell.execute_reply":"2021-08-19T09:25:57.290111Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["(299628, 27)\n"]}],"source":["df = pd.read_csv(\"../input/train_folds.csv\")\n","df_test = pd.read_csv(\"../input/test.csv\")\n","sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n","\n","df = df.drop(df[df['target'].lt(6)].index)\n","print(df.shape)\n","\n","useful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\n","object_cols = [col for col in useful_features if 'cat' in col]\n","numerical_cols = [col for col in useful_features if 'cont' in col]\n","df_test = df_test[useful_features]\n","\n","# polynomial features\n","\n","poly = preprocessing.PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n","train_poly = poly.fit_transform(df[numerical_cols])\n","test_poly = poly.fit_transform(df_test[numerical_cols])\n","\n","\n","\n","df_poly = pd.DataFrame(train_poly, columns= [f\"poly_{i}\" for i in range(train_poly.shape[1])])\n","df_test_poly = pd.DataFrame(test_poly, columns= [f\"poly_{i}\" for i in range(test_poly.shape[1])])\n","\n","df = pd.concat([df, df_poly], axis = 1)\n","df_test = pd.concat([df_test, df_test_poly], axis = 1)\n","\n","useful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\n","object_cols = [col for col in useful_features if 'cat' in col]\n","df_test = df_test[useful_features]\n","# target encoding\n","for col in object_cols:\n","    temp_df = []\n","    temp_test_feat = None\n","    for fold in range(5):\n","        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n","        xvalid = df[df.kfold == fold].reset_index(drop=True)\n","        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n","        feat = feat.to_dict()\n","        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n","        temp_df.append(xvalid)\n","        if temp_test_feat is None:\n","            temp_test_feat = df_test[col].map(feat)\n","        else:\n","            temp_test_feat += df_test[col].map(feat)\n","    \n","    temp_test_feat /= 5\n","    df_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n","    df = pd.concat(temp_df)\n","\n","useful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\n","object_cols = [col for col in useful_features if col.startswith(\"cat\")]\n","df_test = df_test[useful_features]\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def run(trial):\n","\n","    n_estimators = trial.suggest_int(\"n_estimators\", 1000, 8000)\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n","    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n","    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n","    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n","    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n","    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n","\n","\n","\n","\n","    final_predictions = []\n","    scores=[]\n","    for fold in range(5):\n","        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n","        xvalid = df[df.kfold == fold].reset_index(drop=True)\n","        #xtest = df_test.copy()\n","\n","\n","        ytrain = xtrain.target\n","        yvalid = xvalid.target\n","        \n","        xtrain = xtrain[useful_features]\n","        xvalid = xvalid[useful_features]\n","\n","    \n","        # standarization\n","\n","        scaler = preprocessing.StandardScaler()\n","        xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n","        xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n","        #xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n","\n","        # categorical features\n","        high_cardinality_cols = [col for col in object_cols if xtrain[col].nunique()>=9]\n","        low_cardinality_cols = [col for col in object_cols if xtrain[col].nunique()<9]\n","        \n","        \n","        # label encode columns with high cardinality \n","        ordinal_encoder = preprocessing.OrdinalEncoder()\n","        xtrain[high_cardinality_cols] = ordinal_encoder.fit_transform(xtrain[high_cardinality_cols])\n","        xvalid[high_cardinality_cols] = ordinal_encoder.transform(xvalid[high_cardinality_cols])\n","        #xtest[high_cardinality_cols] = ordinal_encoder.transform(xtest[high_cardinality_cols])\n","        \n","        # One hot encode columns with low cardinality \n","        OH_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n","\n","        OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(xtrain[low_cardinality_cols]))\n","        OH_cols_valid = pd.DataFrame(OH_encoder.transform(xvalid[low_cardinality_cols]))\n","        #OH_cols_test = pd.DataFrame(OH_encoder.transform(xtest[low_cardinality_cols]))\n","\n","        # codificador one-hot elimina; ponerlo de nuevo \n","        OH_cols_train.index = xtrain.index\n","        OH_cols_valid.index = xvalid.index\n","        #OH_cols_test.index = xtest.index\n","\n","        # Eliminar columnas categóricas (se reemplazarán con codificación one-hot) \n","        num_X_train = xtrain.drop(low_cardinality_cols, axis=1)\n","        num_X_valid = xvalid.drop(low_cardinality_cols, axis=1)\n","        #num_X_test= xtest.drop(low_cardinality_cols, axis=1)\n","\n","        #  añadir columnas codificadas one-hot a variables numéricas \n","        after_OH_xtrain = pd.concat([num_X_train, OH_cols_train], axis=1)\n","        after_OH_valid= pd.concat([num_X_valid, OH_cols_valid], axis=1)\n","        #after_OH_test= pd.concat([num_X_test, OH_cols_test], axis=1) \n","\n","\n","        model = XGBRegressor(random_state=42, #fold, \n","                            n_jobs=-1,\n","                            n_estimators= n_estimators,\n","                            tree_method='gpu_hist',\n","                            learning_rate= learning_rate,\n","                            subsample= subsample,\n","                            max_depth= max_depth,\n","                            colsample_bytree= colsample_bytree,\n","                            reg_alpha = reg_alpha,\n","                            eval_metric='rmse',\n","                            reg_lambda = reg_lambda,\n","                            gpu_id=0,predictor='gpu_predictor',\n","                            objective='reg:squarederror')\n","        \n","        \n","        model.fit(after_OH_xtrain, ytrain, early_stopping_rounds=300, eval_set=[(after_OH_valid,yvalid)], verbose=1000)\n","        preds_valid = model.predict(after_OH_valid)\n","        #test_preds = model.predict(after_OH_test)\n","        #final_predictions.append(test_preds)\n","        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n","        print(fold,rmse)\n","        scores.append(rmse)\n","        \n","    return np.mean(scores)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2021-08-25 10:47:23,508]\u001b[0m A new study created in memory with name: no-name-ce99017e-69eb-400f-b8af-e26a73afa950\u001b[0m\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'run' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_4247/1953022194.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"]}],"source":["study = optuna.create_study(direction=\"minimize\")\n","study.optimize(run, n_trials=30)\n","\n"]},{"source":["0.7289161215950625  ordinal + stand   <br>\n","0.7359939686055645  ordinal + normalizer <br>\n","0.7339300282189536  ordinal + standard + normalizer <br>\n","0.7359933943841589  ordinal + normalizer +standard   <br>\n","0.7288775872910201  ohe + stand   <br>\n","0.7359376887794242   ohe + normalizer   <br>\n","0.7289403434752502 (ohe+ 3 ordinal) + stand   <br>\n","0.7291070689887855  poly3 (T,F) (ohe+ 1 ordinal) + stand   <br>\n","0.72914235959686  poly3 (F,F) (ohe+ 1 ordinal) + stand   <br>\n","0.728907008813998  poly2 (F,F) (ohe+ 1 ordinal) + stand   <br>\n","0.7289321289479873   poly2 (F,T) (ohe+ 1 ordinal) + stand   <br>\n","0.7289501787229472   poly2 (T,T) (ohe+ 1 ordinal) + stand   <br>\n","0.7289416327232601   poly2 (T,F) (ohe+ 1 ordinal) + stand   <br>\n","\n","0.7288644838881868  (ohe+ 1 ordinal) + stand   <br>\n","-0.7189543356528036   T_outliers+ (ohe+ 1 ordinal) + stand <br>\n","0.7205793549092518    T_encoding + T_outliers+ (ohe+ 1 ordinal) + stand <br>\n"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["study.best_params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}